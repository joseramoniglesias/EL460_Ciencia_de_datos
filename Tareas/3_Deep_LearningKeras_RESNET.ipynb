{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c5e044d",
   "metadata": {},
   "source": [
    "Entrenando la arquitectura ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcbffde",
   "metadata": {},
   "source": [
    "Keras tiene a nuestra disposición la arquitectura ResNet. Vamos a entrenar este modelo. Debido a que vamos a usar la definida en Keras (aunque podríamos crearla directamente), debemos aumentar el tamaño de las imágenes a 48 píxeles. Para ello, usaremos el siguiente código:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4493cb",
   "metadata": {},
   "source": [
    "Importando Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b852f429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import misc\n",
    "from PIL import Image\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "from IPython.display import SVG\n",
    "import cv2\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from keras import layers\n",
    "from keras.layers import Flatten, Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, Dropout\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from keras.utils import layer_utils, np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras import losses\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded1796b",
   "metadata": {},
   "source": [
    "Preparando el conjunto de datos Como antes, usaremos el conjunto de datos CIFAR-100, que, como ya dijimos, consta de 600 imágenes por cada clase de un total de 100 clases. Se divide en 500 imágenes para entrenamiento y 100 imágenes para validación por cada clase. Las 100 clases están agrupadas en 20 superclases. Cada imagen tiene una etiqueta \"fina\" (la clase, de entre las 100, a la que pertenece) y una etiqueta \"gruesa\" (correspondiente a su superclase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1f289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar100\n",
    "\n",
    "(x_train_original, y_train_original), (x_test_original, y_test_original) = cifar100.load_data(label_mode='fine')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45883e2c",
   "metadata": {},
   "source": [
    "Actualmente, hemos descargado los datasets de entrenamiento y validación. x_train_original y x_test_original son los conjuntos de datos con lás imágenes de entrenamiento y validación respectivamente, mientras que y_train_original y y_test_original son los datasets con las etiquetas.\n",
    "\n",
    "Veamos la forma de y_train_original:\n",
    "    array([[19], [29], [ 0], ..., [ 3], [ 7], [73]])  \n",
    "    \n",
    "Como se puede ver, se trata de un array donde cada número se corresponde con la etiqueta concreta. Lo primero que hay que hacer es convertir este array en su versión one-hot-encoding    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f845c088",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train_original, 100)\n",
    "y_test = np_utils.to_categorical(y_test_original, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d980c274",
   "metadata": {},
   "source": [
    "Bien, representa la imagen en los 3 canales RGB de 256 píxeles. Vamos a verla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290c4f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgplot = plt.imshow(x_train_original[3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9520837",
   "metadata": {},
   "source": [
    "Lo que haremos a continuación, es normalizar las imágenes. Esto es, dividiremos cada elemento de x_train_original y xtestoriginal por el numero de píxeles, es decir, 255. Con esto obtenemos que el array comprenderá valores de entre 0 y 1. Con esto el entrenamiento suele aportar mejores resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2ac200",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_original/255  \n",
    "x_test = x_test_original/255  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae24e40",
   "metadata": {},
   "source": [
    "Preparando el entorno\n",
    "El siguiente paso es definir ciertos parámetros sobre el experimento en Keras. Lo primero será especificar a Keras dónde se encuentran los canales. En un array de imágenes, pueden venir como último indice o como el primero. Esto se conoce como canales primero (channels first) o canales al final (channels last). En nuestro caso, vamos a definirlos al final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2250c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_image_data_format('channels_last')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39864914",
   "metadata": {},
   "source": [
    "Lo siguiente que vamos a especificar es la fase del experimento. En este caso, la fase será de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae3751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_learning_phase(True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13578981",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_original[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682d55c4",
   "metadata": {},
   "source": [
    "Entrenando la arquitectura ResNet\n",
    "Keras tiene a nuestra disposición ésta arquitectura, pero tiene el problema que, por defecto, el tamaño de las imágenes debe ser mayor a 187 píxeles, por lo que definiremos una arquitectura más pequeña."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07532c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomResNet50(include_top=True, input_tensor=None, input_shape=(32,32,3), pooling=None, classes=100):  \n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "\n",
    "    x = ZeroPadding2D(padding=(2, 2), name='conv1_pad')(img_input)\n",
    "\n",
    "    x = resnet50.conv_block(x, 3, [32, 32, 64], stage=2, block='a')\n",
    "    x = resnet50.identity_block(x, 3, [32, 32, 64], stage=2, block='b')\n",
    "    x = resnet50.identity_block(x, 3, [32, 32, 64], stage=2, block='c')\n",
    "\n",
    "    x = resnet50.conv_block(x, 3, [64, 64, 256], stage=3, block='a', strides=(1, 1))\n",
    "    x = resnet50.identity_block(x, 3, [64, 64, 256], stage=3, block='b')\n",
    "    x = resnet50.identity_block(x, 3, [64, 64, 256], stage=3, block='c')\n",
    "\n",
    "    x = resnet50.conv_block(x, 3, [128, 128, 512], stage=4, block='a')\n",
    "    x = resnet50.identity_block(x, 3, [128, 128, 512], stage=4, block='b')\n",
    "    x = resnet50.identity_block(x, 3, [128, 128, 512], stage=4, block='c')\n",
    "    x = resnet50.identity_block(x, 3, [128, 128, 512], stage=4, block='d')\n",
    "\n",
    "    x = resnet50.conv_block(x, 3, [256, 256, 1024], stage=5, block='a')\n",
    "    x = resnet50.identity_block(x, 3, [256, 256, 1024], stage=5, block='b')\n",
    "    x = resnet50.identity_block(x, 3, [256, 256, 1024], stage=5, block='c')\n",
    "    x = resnet50.identity_block(x, 3, [256, 256, 1024], stage=5, block='d')\n",
    "    x = resnet50.identity_block(x, 3, [256, 256, 1024], stage=5, block='e')\n",
    "    x = resnet50.identity_block(x, 3, [256, 256, 1024], stage=5, block='f')\n",
    "\n",
    "    x = resnet50.conv_block(x, 3, [512, 512, 2048], stage=6, block='a')\n",
    "    x = resnet50.identity_block(x, 3, [512, 512, 2048], stage=6, block='b')\n",
    "    x = resnet50.identity_block(x, 3, [512, 512, 2048], stage=6, block='c')\n",
    "\n",
    "    x = AveragePooling2D((1, 1), name='avg_pool')(x)\n",
    "\n",
    "    if include_top:\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(classes, activation='softmax', name='fc1000')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "        elif pooling == 'max':\n",
    "            x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = Model(inputs, x, name='resnet50')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a82572",
   "metadata": {},
   "source": [
    "Compilamos como hasta ahora..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbf19aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_resnet50():  \n",
    "  model = CustomResNet50(include_top=True, input_tensor=None, input_shape=(32,32,3), pooling=None, classes=100)\n",
    "\n",
    "  return model\n",
    "\n",
    "custom_resnet50_model = create_custom_resnet50()  \n",
    "custom_resnet50_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc', 'mse'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec991079",
   "metadata": {},
   "source": [
    "Una vez hecho esto, vamos a ver un resumen del modelo creado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0fb9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_resnet50_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef4bf27",
   "metadata": {},
   "source": [
    "Recordemos que la arquitectura VGG-16 tenía aproximadamente 34 millones de parámetros a entrenar. Esto quiere decir que hemos aumentado la profundidad pero hemos reducido el número de parámetros a entrenar.\n",
    "\n",
    "Bien, dicho esto, pasamos a entrenar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a5f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "crn50 = custom_resnet50_model.fit(x=x_train, y=y_train, batch_size=32, epochs=10, verbose=1, validation_data=(x_test, y_test), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f02b461",
   "metadata": {},
   "source": [
    "Veamos las métricas obtenidas para el entrenamiento y validación gráficamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f812257",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)  \n",
    "plt.plot(crn50.history['acc'],'r')  \n",
    "plt.plot(crn50.history['val_acc'],'g')  \n",
    "plt.xticks(np.arange(0, 11, 2.0))  \n",
    "plt.rcParams['figure.figsize'] = (8, 6)  \n",
    "plt.xlabel(\"Num of Epochs\")  \n",
    "plt.ylabel(\"Accuracy\")  \n",
    "plt.title(\"Training Accuracy vs Validation Accuracy\")  \n",
    "plt.legend(['train','validation'])\n",
    "\n",
    "plt.figure(1)  \n",
    "plt.plot(crn50.history['loss'],'r')  \n",
    "plt.plot(crn50.history['val_loss'],'g')  \n",
    "plt.xticks(np.arange(0, 11, 2.0))  \n",
    "plt.rcParams['figure.figsize'] = (8, 6)  \n",
    "plt.xlabel(\"Num of Epochs\")  \n",
    "plt.ylabel(\"Loss\")  \n",
    "plt.title(\"Training Loss vs Validation Loss\")  \n",
    "plt.legend(['train','validation'])\n",
    "\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d6d28a",
   "metadata": {},
   "source": [
    "Matriz de confusión\n",
    "Pasemos ahora a ver la matriz de confusión y las métricas de Accuracy, Recall y F1-score.\n",
    "\n",
    "Vamos a hacer una predicción sobre el dataset de validación y, a partir de ésta, generamos la matriz de confusión y mostramos las métricas mencionadas anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b339e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "crn50_pred = custom_resnet50_model.predict(x_test, batch_size=32, verbose=1)  \n",
    "crn50_predicted = np.argmax(crn50_pred, axis=1)\n",
    "\n",
    "crn50_cm = confusion_matrix(np.argmax(y_test, axis=1), crn50_predicted)\n",
    "\n",
    "# Visualizing of confusion matrix\n",
    "crn50_df_cm = pd.DataFrame(crn50_cm, range(100), range(100))  \n",
    "plt.figure(figsize = (20,14))  \n",
    "sn.set(font_scale=1.4) #for label size  \n",
    "sn.heatmap(crn50_df_cm, annot=True, annot_kws={\"size\": 12}) # font size  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5749e71a",
   "metadata": {},
   "source": [
    "Y por último, mostramos las métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e38738",
   "metadata": {},
   "outputs": [],
   "source": [
    "crn50_report = classification_report(np.argmax(y_test, axis=1), crn50_predicted)  \n",
    "print(crn50_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b18698",
   "metadata": {},
   "source": [
    "Curva ROC (tasas de verdaderos positivos y falsos positivos)\n",
    "Vamos a codificar la curva ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a79b585",
   "metadata": {},
   "outputs": [],
   "source": [
    "rom sklearn.datasets import make_classification  \n",
    "from sklearn.preprocessing import label_binarize  \n",
    "from scipy import interp  \n",
    "from itertools import cycle\n",
    "\n",
    "n_classes = 100\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Plot linewidth.\n",
    "lw = 2\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()  \n",
    "tpr = dict()  \n",
    "roc_auc = dict()  \n",
    "for i in range(n_classes):  \n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], crn50_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), crn50_pred.ravel())  \n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)  \n",
    "for i in range(n_classes):  \n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr  \n",
    "tpr[\"macro\"] = mean_tpr  \n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(1)  \n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],  \n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],  \n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])  \n",
    "for i, color in zip(range(n_classes-97), colors):  \n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)  \n",
    "plt.xlim([0.0, 1.0])  \n",
    "plt.ylim([0.0, 1.05])  \n",
    "plt.xlabel('False Positive Rate')  \n",
    "plt.ylabel('True Positive Rate')  \n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')  \n",
    "plt.legend(loc=\"lower right\")  \n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Zoom in view of the upper left corner.\n",
    "plt.figure(2)  \n",
    "plt.xlim(0, 0.2)  \n",
    "plt.ylim(0.8, 1)  \n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],  \n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],  \n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])  \n",
    "for i, color in zip(range(10), colors):  \n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)  \n",
    "plt.xlabel('False Positive Rate')  \n",
    "plt.ylabel('True Positive Rate')  \n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')  \n",
    "plt.legend(loc=\"lower right\")  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82159cf3",
   "metadata": {},
   "source": [
    "Salvaremos los datos del histórico de entrenamiento para compararlos con otros modelos. Además, vamos a salvar el modelo con los pesos entrenados para usarlos en el futuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c993863f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo\n",
    "custom_resnet50_model.save('crn50.h5')\n",
    "\n",
    "#Histórico\n",
    "with open('crn50_history.txt', 'wb') as file_pi:  \n",
    "  pickle.dump(crn50.history, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6d4e2d",
   "metadata": {},
   "source": [
    "A continuación, vamos a comparar las métricas con los modelos anteriores (obviaremos el código que carga los datos de dichos modelos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b3293b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)  \n",
    "plt.plot(snn.history['val_acc'],'r')  \n",
    "plt.plot(scnn.history['val_acc'],'g')  \n",
    "plt.plot(vgg16.history['val_acc'],'b')  \n",
    "plt.plot(vgg19.history['val_acc'],'y')  \n",
    "plt.plot(vgg16Bis.history['val_acc'],'m')  \n",
    "plt.plot(crn50.history['val_acc'],'gold')  \n",
    "plt.xticks(np.arange(0, 11, 2.0))  \n",
    "plt.rcParams['figure.figsize'] = (8, 6)  \n",
    "plt.xlabel(\"Num of Epochs\")  \n",
    "plt.ylabel(\"Accuracy\")  \n",
    "plt.title(\"Simple NN Accuracy vs simple CNN Accuracy\")  \n",
    "plt.legend(['simple NN','CNN','VGG 16','VGG 19','Custom VGG','Custom ResNet'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d6e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)  \n",
    "plt.plot(snn.history['val_loss'],'r')  \n",
    "plt.plot(scnn.history['val_loss'],'g')  \n",
    "plt.plot(vgg16.history['val_loss'],'b')  \n",
    "plt.plot(vgg19.history['val_loss'],'y')  \n",
    "plt.plot(vgg16Bis.history['val_loss'],'m')  \n",
    "plt.plot(crn50.history['val_loss'],'gold')  \n",
    "plt.xticks(np.arange(0, 11, 2.0))  \n",
    "plt.rcParams['figure.figsize'] = (8, 6)  \n",
    "plt.xlabel(\"Num of Epochs\")  \n",
    "plt.ylabel(\"Loss\")  \n",
    "plt.title(\"Simple NN Loss vs simple CNN Loss\")  \n",
    "plt.legend(['simple NN','CNN','VGG 16','VGG 19','Custom VGG','Custom ResNet'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db73c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)  \n",
    "plt.plot(snn.history['val_mean_squared_error'],'r')  \n",
    "plt.plot(scnn.history['val_mean_squared_error'],'g')  \n",
    "plt.plot(vgg16.history['val_mean_squared_error'],'b')  \n",
    "plt.plot(vgg19.history['val_mean_squared_error'],'y')  \n",
    "plt.plot(vgg16Bis.history['val_mean_squared_error'],'m')  \n",
    "plt.plot(crn50.history['val_mean_squared_error'],'gold')  \n",
    "plt.xticks(np.arange(0, 11, 2.0))  \n",
    "plt.rcParams['figure.figsize'] = (8, 6)  \n",
    "plt.xlabel(\"Num of Epochs\")  \n",
    "plt.ylabel(\"Mean Squared Error\")  \n",
    "plt.title(\"Simple NN MSE vs simple CNN MSE\")  \n",
    "plt.legend(['simple NN','CNN','VGG 16','VGG 19','Custom VGG','Custom ResNet'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7f84f0",
   "metadata": {},
   "source": [
    "Conclusión sobre el trabajo realizado\n",
    "Como se puede ver, la arquitectura marca un punto de inflexión. No sólo porque sea de los mejores resultados que las anteriores arquitecturas, sino también en los tiempos de entrenamiento, ya que permite aumentar las capas con un tiempo aceptable; y también en el número de parámetros, que se ha reducido considerablemente respecto a la arquitectura VGG."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
